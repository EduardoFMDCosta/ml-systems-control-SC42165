{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e32b319",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks: Learning the Dubins car dynamics\n",
    "\n",
    "In this notebook, we aim to **learn the dynamics of the Dubins car using a fully connected neural network** in Python. For this, we will employ the Pytorch library, which is an optimized tensor library for deep learning using GPUs and CPUs (https://pytorch.org/). \n",
    "\n",
    "Let $h$ be a step size, $v$ a velocity, and a constant control $u$. The Dubins car model (in discrete time) can be described as:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{t+1} = f(x_t) = \\begin{bmatrix}\n",
    "                      x_{t, 1} + h v \\sin(x_{t, 3}) \\\\\n",
    "                      x_{t, 2} + h v \\cos(x_{t, 3}) \\\\\n",
    "                      x_{t, 3} + h u\n",
    "                    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $x_{t, 1}$ represents the $x$-position of the car at instant $t$, $x_{t, 2}$ the $y$-position, and $x_{t, 3}$ is its steering angle. Note that this dynamics is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9bbab",
   "metadata": {},
   "source": [
    "### 1) What we want to see\n",
    "\n",
    "- **Understand the first sections and answer questions**: The first 6 sections contain explanations about the dataset and the construction of a simple NN. There are <span style=\"color: #8080ff\">some questions in blue</span> that should be answered (one or two sentences are more than enough :) ) \n",
    "\n",
    "- **Code Section 7**: Section 7 is the main task of this exercise. It requires the construction of a larger NN, also varying training choices \n",
    "\n",
    "- You can do everything on your laptop, but it might be a good idea to use google colab (https://colab.research.google.com/) to get free access to a GPU\n",
    "\n",
    "- You will send your notebook containing the results (WE WON'T RUN YOUR NOTEBOOK) as a unique jupyter notebook file (.ipynb extension). We won't open any other supplementary file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916634f9",
   "metadata": {},
   "source": [
    "### 2) Loading required libraries\n",
    "\n",
    "First, we load the packages we want to use. This includes Numpy for preprocessing the data, Pytorch (torch) for building and training the neural networks, and Matplotlib visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d7a1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce823ebb",
   "metadata": {},
   "source": [
    "### 3) Loading the dataset\n",
    "\n",
    "To build our dataset, we selected the values for the parameters $h = 0.3$, $v = 5$, and $u = 2$ and generated a dataset $\\mathcal{D} = \\{ x_{t}, x_{t+1}  \\}_{t=0}^{N}$ by simulating the system for multiple (randomly selected) initial positions for 20 time steps. In order words, we use the following routine to build the set:\n",
    "\n",
    "For $M$ runs:\n",
    "\n",
    "- Select randomly $x_0$ according to some probability distribution\n",
    "\n",
    "- Use equation (1) for $H$ steps (we use $H = 20$ in this exercise)\n",
    "\n",
    "- Store the trajectory as $\\{ (x_{0}, x_{1}), ..., (x_{H-1}, x_{H}) \\}$\n",
    "\n",
    "- Include the trajectory in the dataset $\\mathcal{D}$\n",
    "\n",
    "\n",
    "In the end, our dataset $\\mathcal{D}$ will contain $MH$ data pairs $(x_{t}, x_{t+1})$, which we will further denote by $(x, y)$ (see, for instance, the dataset columns' names).\n",
    "\n",
    "For more details, we have also made available the notebook *DatasetGeneration.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81d15c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read CSV file containing data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "#Deserialize the strings back into NumPy arrays\n",
    "data['x'] = data['x'].apply(lambda x: np.array(x.split(), dtype=float))\n",
    "data['y'] = data['y'].apply(lambda x: np.array(x.split(), dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c997b20",
   "metadata": {},
   "source": [
    "Below, we display the first observations of the dataset. Note how each value can be represented by a vector in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "152edb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3101802085903067, -2.7617156216425283, -0.9...</td>\n",
       "      <td>[-0.8985558258519621, -1.873482994995219, -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.2107662316485737, -0.518452332717056, -2.02...</td>\n",
       "      <td>[-0.13627455041796743, -1.1783622746406195, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.8964583531063366, -2.3254043134674305, -2....</td>\n",
       "      <td>[-2.708374788860225, -3.586670225697164, -1.96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-2.5967448061008858, -2.726547485741719, 0.68...</td>\n",
       "      <td>[-1.6451114237123, -1.5670674705954208, 1.2872...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-2.4125823366086436, 2.487834736931684, -1.95...</td>\n",
       "      <td>[-3.8019700085300654, 1.9225012980544394, -1.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  [0.3101802085903067, -2.7617156216425283, -0.9...   \n",
       "1  [1.2107662316485737, -0.518452332717056, -2.02...   \n",
       "2  [-1.8964583531063366, -2.3254043134674305, -2....   \n",
       "3  [-2.5967448061008858, -2.726547485741719, 0.68...   \n",
       "4  [-2.4125823366086436, 2.487834736931684, -1.95...   \n",
       "\n",
       "                                                   y  \n",
       "0  [-0.8985558258519621, -1.873482994995219, -0.3...  \n",
       "1  [-0.13627455041796743, -1.1783622746406195, -1...  \n",
       "2  [-2.708374788860225, -3.586670225697164, -1.96...  \n",
       "3  [-1.6451114237123, -1.5670674705954208, 1.2872...  \n",
       "4  [-3.8019700085300654, 1.9225012980544394, -1.3...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display dataset (5 first observations)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d03693",
   "metadata": {},
   "source": [
    "#### 3.1) Splitting the dataset into training and testing sets\n",
    "\n",
    "With the help of the Scikit-Learn package (https://scikit-learn.org/stable/) - one of the most famous and useful ML libraries available -, we use the *train_test_split* method to split our dataset in two: 80% of the observations (randomly selected) are included in the training set (used to calibrate the weights of the NN), while the remaining 20% become part of the testing set (used to measure the accuracy of the model on unseen data). Those proportions can be modified by changing the parameter *test_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6f9341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Splits dataset randomly into testing (20% of observations) and training (80%)\n",
    "train_set, test_set = train_test_split(data, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#TRAINING\n",
    "x_train = train_set['x']\n",
    "y_train = train_set['y']\n",
    "\n",
    "#TESTING\n",
    "x_test = test_set['x']\n",
    "y_test = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96c4c0",
   "metadata": {},
   "source": [
    "We convert the training and testing sets to Pytorch tensors, which is the data structure handle by Pytorch for NN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7024a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = x_train.apply(lambda x: torch.tensor(x)).tolist()\n",
    "x_train = torch.stack(tensor_list).to(dtype=torch.float32)\n",
    "\n",
    "tensor_list = y_train.apply(lambda x: torch.tensor(x)).tolist()\n",
    "y_train = torch.stack(tensor_list).to(dtype=torch.float32)\n",
    "\n",
    "tensor_list = x_test.apply(lambda x: torch.tensor(x)).tolist()\n",
    "x_test = torch.stack(tensor_list).to(dtype=torch.float32)\n",
    "\n",
    "tensor_list = y_test.apply(lambda x: torch.tensor(x)).tolist()\n",
    "y_test = torch.stack(tensor_list).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d22a5b",
   "metadata": {},
   "source": [
    "We can check the shape (dimensions) of the tensors as below. \n",
    "\n",
    "<span style=\"color: #8080ff\">**QUESTION 1**: </span> What each dimension of the tensors mean/represent?\n",
    "\n",
    "**Answer:** In our case, the first dimension represents the number of samples (observations), while the second represents the dimension of each sample (a vector in $\\mathbb{R}^3$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8396e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([160000, 3])\n",
      "x_test shape: torch.Size([40000, 3])\n",
      "y_train shape: torch.Size([160000, 3])\n",
      "y_test shape: torch.Size([40000, 3])\n",
      "160000 train samples\n",
      "40000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30956c95",
   "metadata": {},
   "source": [
    "### 4) Building our first (simple and small) neural network\n",
    "The goal is now to create a **dense feedforward neural network** (hereafter denoted by $f_{\\text{NN}}$) which can represent the Dubins car dynamics. In other words, we would like that, for any input $x \\in \\mathbb{R}^3$, $f_{\\text{NN}}(w, x) \\approx f(x)$ (where $w$ represents the weights of the NN).\n",
    "\n",
    "<br>\n",
    "Such network consists out of three parts, which are the <span style=\"color: #ff8080\">input layer</span>, the <span style=\"color: #8080ff\">hidden layers</span> and the <span style=\"color: #80ff80\">output layer</span> (as illustrated by the figure below).\n",
    "\n",
    "![An examplar structure of a feed-forward neural network](Neural_Network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793fd4e",
   "metadata": {},
   "source": [
    "<span style=\"color: #8080ff\">**QUESTION 2**: </span> In our case, how many neurons should the input layer have? How about the output layer? How about the hidden layers, are there any restrictions on the number of neurons?\n",
    "\n",
    "**Answer:** In our case, the input layer should have 3 neurons (as $x \\in \\mathbb{R}^3$). The engineer can then choose the number of hidden layers, as well as the number of neurons in each one of them. Then, the output layer must also have 3 neurons (as $y \\in \\mathbb{R}^3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005fa12",
   "metadata": {},
   "source": [
    "<span style=\"color: #8080ff\">**QUESTION 3**: </span> We mentioned that we are going to build a dense feedforward NN. What does *feedforward* mean (you can explain this by given an example of a structure that is not feedforward)? What does *dense* mean?\n",
    "\n",
    "**Answer:** The term *forward* means that the information flows from input to output (passing through the hidden layers) in a uni-directional fashion (see *Recurrent NNs* for an example in which this is not the case), while *dense* means that we have connections (weights) between all the neurons from one layer to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f0272",
   "metadata": {},
   "source": [
    "In the Pytorch framework, we build the NN as a class, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c64b0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_network(nn.Module): # inherit the nn.Module class for backpropagation and training functionalities\n",
    "    \n",
    "    #Build the layers of the network, and initializes the parameters\n",
    "    def __init__(self): \n",
    "        super(Neural_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10, bias = True)  # fully connected layer from 3 to 10 dimensions\n",
    "        self.fc2 = nn.Linear(10, 3, bias = True) # fully connected layer from 10 to 3 dimensions\n",
    "    \n",
    "    #Build the forward call \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim = 1) # x is reshaped into a n x 3 dimensional input (actually not needed in our case, but usually useful)\n",
    "        x = self.fc1(x) # apply the first fully connected layer, x now has shape n x 10\n",
    "        x = F.relu(x) # apply a ReLU activation to the hidden layer\n",
    "        x = self.fc2(x) # apply the second fully connected layer, x now has shape n x 3\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376480c4",
   "metadata": {},
   "source": [
    "The parts of the model, as well as their respective parameters, can then be displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcabc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_network(\n",
      "  (fc1): Linear(in_features=3, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameter arrays: 4\n",
      "The shape of the parameter arrays:\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # set random seed for variabl initialization\n",
    "\n",
    "net = Neural_network() # initialize the NN\n",
    "print(net)\n",
    "print('')\n",
    "\n",
    "params = list(net.parameters())\n",
    "print('Number of parameter arrays: ' + str(len(params)))\n",
    "print('The shape of the parameter arrays:')\n",
    "\n",
    "for param in params:\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb01ffd",
   "metadata": {},
   "source": [
    "#### 4.1) Training the model\n",
    "After writing the model class, we can now train the parameters of the model on the training dataset. Here, we have to do a number of steps. \n",
    "\n",
    "##### 4.1.1) Defining the loss function\n",
    "\n",
    "The loss function captures the distance between an actual output and the predicted output by the neural network. In our case, we are in a regression setting, and we propose to use the MSE loss (https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss). More precisely, given a dataset $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^{N}$, we define \n",
    "\n",
    "\\begin{equation}\n",
    "L(w; \\mathcal{D}) := \\frac{1}{N} \\sum_{i=1}^{N} ||y_i - f_{\\text{NN}}(w, x_i)||^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "634b7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386e01d",
   "metadata": {},
   "source": [
    "##### 4.1.2) Defining the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8aa2d",
   "metadata": {},
   "source": [
    "Gradient descent is usually the optimization technique used to minimize the loss. Once we have computed the gradient of the loss $L$ with respect to the weights $w$, there are several ways in which we can update $w$. Pytorch offers several optimizer algorithms for such a task  (https://pytorch.org/docs/stable/optim.html): Adadelta, Adagrad, Adam, etc. \n",
    "\n",
    "This paper (https://arxiv.org/pdf/1412.6980.pdf) introduces the Adam algorithm, which we employ in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0f88c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr = 0.001, betas = (0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69744f66",
   "metadata": {},
   "source": [
    "##### 4.1.3) Training routine\n",
    "\n",
    "We describe the training routine as follows:\n",
    "\n",
    "- Subdivide the training set in (randomly selected) batches of size *batch_size*. The batch $k$ will be represented by $\\mathcal{B}_{k} = \\{ (x_i, y_i) \\}_{i=1}^{B}$, and contains $B$ randomly selected samples from $\\mathcal{D}$.\n",
    "\n",
    "- For each batch $\\mathcal{B}_{k}$, compute the gradient of the loss $L(w; \\mathcal{B}_{k})$ with respect to $w$. Note that we are considering the loss on the batch $\\mathcal{B}_{k}$, and not on the whole training set $\\mathcal{D}$.\n",
    "\n",
    "    - Update the weights $w$ according to the rule defined by the optimizer. You can think of this process as some kind of weight update $w \\gets w - \\eta \\nabla_{w} L(w; \\mathcal{B}_{k})$.\n",
    "\n",
    "\n",
    "\n",
    "Note that this process is repeated for a number of epochs, which is just a fancy name for number of times that we pass through the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a309b43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 1/100: 1.4252e+00\n",
      "Loss for epoch 2/100: 4.6781e-01\n",
      "Loss for epoch 3/100: 4.5459e-01\n",
      "Loss for epoch 4/100: 4.4754e-01\n",
      "Loss for epoch 5/100: 4.4233e-01\n",
      "Loss for epoch 6/100: 4.3760e-01\n",
      "Loss for epoch 7/100: 4.3267e-01\n",
      "Loss for epoch 8/100: 4.2841e-01\n",
      "Loss for epoch 9/100: 4.2390e-01\n",
      "Loss for epoch 10/100: 4.1924e-01\n",
      "Loss for epoch 11/100: 4.1471e-01\n",
      "Loss for epoch 12/100: 4.1067e-01\n",
      "Loss for epoch 13/100: 4.0683e-01\n",
      "Loss for epoch 14/100: 4.0379e-01\n",
      "Loss for epoch 15/100: 4.0072e-01\n",
      "Loss for epoch 16/100: 3.9751e-01\n",
      "Loss for epoch 17/100: 3.9462e-01\n",
      "Loss for epoch 18/100: 3.9035e-01\n",
      "Loss for epoch 19/100: 3.8230e-01\n",
      "Loss for epoch 20/100: 3.3883e-01\n",
      "Loss for epoch 21/100: 3.1729e-01\n",
      "Loss for epoch 22/100: 3.0841e-01\n",
      "Loss for epoch 23/100: 3.0195e-01\n",
      "Loss for epoch 24/100: 2.9662e-01\n",
      "Loss for epoch 25/100: 2.9323e-01\n",
      "Loss for epoch 26/100: 2.9077e-01\n",
      "Loss for epoch 27/100: 2.8862e-01\n",
      "Loss for epoch 28/100: 2.8665e-01\n",
      "Loss for epoch 29/100: 2.8531e-01\n",
      "Loss for epoch 30/100: 2.8386e-01\n",
      "Loss for epoch 31/100: 2.8264e-01\n",
      "Loss for epoch 32/100: 2.8156e-01\n",
      "Loss for epoch 33/100: 2.8021e-01\n",
      "Loss for epoch 34/100: 2.7943e-01\n",
      "Loss for epoch 35/100: 2.7853e-01\n",
      "Loss for epoch 36/100: 2.7747e-01\n",
      "Loss for epoch 37/100: 2.7671e-01\n",
      "Loss for epoch 38/100: 2.7568e-01\n",
      "Loss for epoch 39/100: 2.7502e-01\n",
      "Loss for epoch 40/100: 2.7411e-01\n",
      "Loss for epoch 41/100: 2.7325e-01\n",
      "Loss for epoch 42/100: 2.7223e-01\n",
      "Loss for epoch 43/100: 2.7158e-01\n",
      "Loss for epoch 44/100: 2.7078e-01\n",
      "Loss for epoch 45/100: 2.6991e-01\n",
      "Loss for epoch 46/100: 2.6957e-01\n",
      "Loss for epoch 47/100: 2.6831e-01\n",
      "Loss for epoch 48/100: 2.6750e-01\n",
      "Loss for epoch 49/100: 2.6692e-01\n",
      "Loss for epoch 50/100: 2.6613e-01\n",
      "Loss for epoch 51/100: 2.6532e-01\n",
      "Loss for epoch 52/100: 2.6427e-01\n",
      "Loss for epoch 53/100: 2.6301e-01\n",
      "Loss for epoch 54/100: 2.6208e-01\n",
      "Loss for epoch 55/100: 2.6084e-01\n",
      "Loss for epoch 56/100: 2.5753e-01\n",
      "Loss for epoch 57/100: 2.4872e-01\n",
      "Loss for epoch 58/100: 2.3285e-01\n",
      "Loss for epoch 59/100: 2.0702e-01\n",
      "Loss for epoch 60/100: 1.9041e-01\n",
      "Loss for epoch 61/100: 1.7805e-01\n",
      "Loss for epoch 62/100: 1.6817e-01\n",
      "Loss for epoch 63/100: 1.5961e-01\n",
      "Loss for epoch 64/100: 1.5217e-01\n",
      "Loss for epoch 65/100: 1.4599e-01\n",
      "Loss for epoch 66/100: 1.4034e-01\n",
      "Loss for epoch 67/100: 1.3516e-01\n",
      "Loss for epoch 68/100: 1.2963e-01\n",
      "Loss for epoch 69/100: 1.2455e-01\n",
      "Loss for epoch 70/100: 1.1995e-01\n",
      "Loss for epoch 71/100: 1.1591e-01\n",
      "Loss for epoch 72/100: 1.1257e-01\n",
      "Loss for epoch 73/100: 1.0940e-01\n",
      "Loss for epoch 74/100: 1.0623e-01\n",
      "Loss for epoch 75/100: 1.0319e-01\n",
      "Loss for epoch 76/100: 1.0037e-01\n",
      "Loss for epoch 77/100: 9.7490e-02\n",
      "Loss for epoch 78/100: 9.4597e-02\n",
      "Loss for epoch 79/100: 9.1580e-02\n",
      "Loss for epoch 80/100: 8.8254e-02\n",
      "Loss for epoch 81/100: 8.5191e-02\n",
      "Loss for epoch 82/100: 8.2256e-02\n",
      "Loss for epoch 83/100: 7.9603e-02\n",
      "Loss for epoch 84/100: 7.7347e-02\n",
      "Loss for epoch 85/100: 7.5183e-02\n",
      "Loss for epoch 86/100: 7.3074e-02\n",
      "Loss for epoch 87/100: 7.0667e-02\n",
      "Loss for epoch 88/100: 6.7970e-02\n",
      "Loss for epoch 89/100: 6.4932e-02\n",
      "Loss for epoch 90/100: 6.1657e-02\n",
      "Loss for epoch 91/100: 5.7186e-02\n",
      "Loss for epoch 92/100: 5.2595e-02\n",
      "Loss for epoch 93/100: 4.8920e-02\n",
      "Loss for epoch 94/100: 4.6363e-02\n",
      "Loss for epoch 95/100: 4.4478e-02\n",
      "Loss for epoch 96/100: 4.3078e-02\n",
      "Loss for epoch 97/100: 4.1891e-02\n",
      "Loss for epoch 98/100: 4.0833e-02\n",
      "Loss for epoch 99/100: 4.0097e-02\n",
      "Loss for epoch 100/100: 3.9223e-02\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 # how many times we want to go through the whole data set\n",
    "batch_size = 64 # how many samples do we process before updating weights\n",
    "batches = int(np.floor(len(y_train)/batch_size)) # how many batches are there when dividing the whole data set\n",
    "\n",
    "net.train() # set network to training mode\n",
    "Index = np.arange(len(y_train)) #Index, so we can randomly shuffle inputs and outputs\n",
    "\n",
    "np.random.seed(0) # set random seed for shuffling\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    np.random.shuffle(Index) # shuffle indices, so batches have randomly selected samples\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        Index_batch = Index[batch * batch_size:(batch + 1) * batch_size]\n",
    "        x_batch = x_train[Index_batch] # Get respective input data\n",
    "        y_batch = y_train[Index_batch] # Get respective output data\n",
    "        \n",
    "        # delete gradients from optimizer (otherwise, gradients are cummulative summed up over all previous batches)\n",
    "        optimizer.zero_grad()\n",
    "        # predict the output for the given inputs (forward pass)\n",
    "        y_batch_pred = net(x_batch)\n",
    "        # calculate the loss of the predicted input (forward pass)\n",
    "        loss = loss_func(y_batch_pred, y_batch)\n",
    "        # get the gradients of the trainable paramters for the given loss (backward pass)\n",
    "        loss.backward()\n",
    "        # apply the gradients and change weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss\n",
    "    \n",
    "    loss_epoch /= batches\n",
    "    print('Loss for epoch {}/{}: {:0.4e}'.format(epoch,epochs, loss_epoch) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ca335",
   "metadata": {},
   "source": [
    "### 5) Testing the model\n",
    "Finally, the model has to be tested in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f25e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval() # Set model inot evaluation mode\n",
    "with torch.no_grad(): # Only build forwards graph => faster method\n",
    "    y_test_pred = net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c59d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on testing set: tensor(0.0387)\n"
     ]
    }
   ],
   "source": [
    "loss_test = loss_func(y_test, y_test_pred)\n",
    "\n",
    "print('Loss on testing set:', loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892907b",
   "metadata": {},
   "source": [
    "<span style=\"color: #8080ff\">**QUESTION 4**: </span> Why does it make sense to measure the accuracy of the model on the test set?\n",
    "\n",
    "**Answer:** This allows us to measure the accuracy on data which was not seen during the training phase. Having good accuracy on the test set may suggest some sort of generalization capacity from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df8587",
   "metadata": {},
   "source": [
    "### 6) Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edc4e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import h, v, u\n",
    "\n",
    "h = h\n",
    "v = v\n",
    "u = u\n",
    "\n",
    "params = [h, v, u]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cda6ff",
   "metadata": {},
   "source": [
    "We run the dynamics for $H$ steps using the true dynamics (blue points) and the predictions coming from our trained NN (red points) for a prespecified initial point $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d34f0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "def plotActualDynamicsAndNNPredictions(initial_state):\n",
    "\n",
    "    state_actual = initial_state\n",
    "    state_nn = torch.Tensor(initial_state)\n",
    "\n",
    "    for t in range(10):\n",
    "\n",
    "        #True dynamics\n",
    "        state_actual = utils.systemDynamics(state_actual, params)\n",
    "        plt.scatter(state_actual[0][0], state_actual[0][1], color = 'blue')\n",
    "\n",
    "        #NN dynamics\n",
    "        with torch.no_grad():\n",
    "            state_nn = net(state_nn)\n",
    "\n",
    "        plt.scatter(state_nn[0][0], state_nn[0][1], color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c302e",
   "metadata": {},
   "source": [
    "#### 6.1) For $x_0 = [1.0, 1.0, 0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "318118d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_state = np.array([[1.0, 1.0, 0.5]])\n",
    "plotActualDynamicsAndNNPredictions(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d106dc9",
   "metadata": {},
   "source": [
    "#### 6.2) For $x_0 = [5.0, 8.0, 1.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed8dfccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhpElEQVR4nO3df2yV5f3/8dfNYVSiPUdgkB16DhSZWC1i3DRbxRPL/LEYRjpPyhzWDan/bOmk1biILk6Jw2K2kDb5JAR0QRZHydYdmC4hTJaBNc5Y1DlwmYA6actZSBbtKRqP2d37+8f9LXBsKz3tdc51fjwfSdOc61zlvO8g3q9e9329b8fzPE8AAAAGTLNdAAAAKB0ECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGTM/3Bw4PD+vUqVOqrKyU4zj5/ngAADAJnudpaGhI8+fP17Rp469L5D1YnDp1StFoNN8fCwAADOjr61MkEhn3/bwHi8rKSkl+YcFgMN8fDwAAJiGVSikajZ49j48n78Fi5PJHMBgkWAAAUGQudBsDN28CAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjMl7gywAKHquK/X0SMmkFA5LsZgUCNiuCigIBAsAyEYiIbW2Sv3958YiEamzU4rH7dUFFAguhQDARCUSUmNjZqiQpIEBfzyRsFPXBLiudPCg1NXlf3dd2xWhVBEsAGAiXNdfqfC80e+NjLW1FeQZO5GQqqulFSuku+7yv1dXF3QOQhEjWADARPT0jF6pOJ/nSX19/rwCUsSLLChSBAsAmIhk0uy8PCjiRRYUMYIFAExEOGx2Xh4U6SILihzBAgAmIhbzd384ztjvO44UjfrzCkQRLrKgBBAsAGAiAgF/S6k0OlyMvO7oKKh+FkW4yIISQLAAgImKx6XubqmqKnM8EvHHC6yPRREusqAEZBUsXNfVo48+qkWLFmnmzJlavHixnnjiCXlj3RkEAKUoHpf+/W/pr3+Vdu3yv7//fsGFCqkoF1lQArLqvPnUU09p69at2rlzp2pra3X48GGtW7dOoVBI69evz1WNAFBYAgGpvt52FRMyssgyVrPQjo6CzEMoclkFi1deeUUNDQ1auXKlJKm6ulpdXV167bXXclIcAGDq4nGpoYHHmyA/sgoWN9xwg7Zv365jx45pyZIleuutt/Tyyy9ry5Yt4/5MOp1WOp0++zqVSk2+WgDApBTRIguKXFbBYsOGDUqlUqqpqVEgEJDrutq0aZOamprG/Zn29nZt3LhxyoUCAIDCl9XNm7/73e/029/+Vrt27dIbb7yhnTt36le/+pV27tw57s88/PDDGhwcPPvV19c35aIBAEBhcrwstnREo1Ft2LBBLS0tZ8d+8Ytf6LnnntO//vWvCf0ZqVRKoVBIg4ODCgaD2VcMAADybqLn76xWLD755BNNm5b5I4FAQMPDw5OrEgAAlJSs7rFYtWqVNm3apAULFqi2tlZvvvmmtmzZoubm5lzVBwAAikhWl0KGhob06KOPas+ePTp9+rTmz5+vNWvW6Oc//7lmzJgxoT+DSyEAABSfiZ6/swoWJhAsAAAoPjm5xwIAAOCLECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMVk9hAxAcXBdqadHSialcFiKxaRAwHZVAMoBwQIoMYmE1Noq9fefG4tEpM5OKR63VxeA8sClEKCEJBJSY2NmqJCkgQF/PJGwUxeA8kGwAEqE6/orFWM9r3hkrK3NnwcAuUKwAEpET8/olYrzeZ7U1+fPA4BcIVgAJSKZNDsPACaDYAGUiHDY7DwAmAyCBVAiYjF/94fjjP2+40jRqD8PAHKFYAGUiEDA31IqjQ4XI687OuhnASC3CBZACYnHpe5uqaoqczwS8cfpYwEg12iQBZSYeFxqaKDzJgA7CBZACQoEpPp621UAKEdcCgEAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ4MsAAAmwHXpaDsRBAsAyAXOQiUlkZBaW6X+/nNjkYj/4D+ewZOJSyEAYFoiIVVXSytWSHfd5X+vrvbHUXQSCamxMTNUSNLAgD/OX2smggUAmMRZqKS4rr9S4Xmj3xsZa2vz58FHsAAAUzgLlZyentEZ8XyeJ/X1+fPgI1gAgCmchUpOMml2XjkgWACAKZyFSk44bHZeOSBYAIApnIVKTizm7/5wnLHfdxwpGvXnwUewAABTOAuVnEDA31Iqjf5rHXnd0cFO4vMRLADAFM5CJSkel7q7paqqzPFIxB+nj0Umx/PGun05d1KplEKhkAYHBxUMBvP50QCQH2N1U4pG/VDBWaholXvPs4mevwkWAJAL5X4WQsmZ6Pmblt4AkAuBgFRfb7sKIO+4xwIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGZBUsqqur5TjOqK+WlpZc1QcAAIpIVs8K6e3tleu6Z18fPXpUt956q1avXm28MAAAUHyyChZz587NeL1582YtXrxYN910k9GiAABAcZr0000/++wzPffcc3rggQfkOM6489LptNLp9NnXqVRqsh8JAAAK3KRv3ty7d68++ugj3XPPPV84r729XaFQ6OxXNBqd7EcCAIAC53ie503mB7/97W9rxowZeuGFF75w3lgrFtFoVIODgwoGg5P5aAAAkGepVEqhUOiC5+9JXQr54IMPdODAASUSiQvOraioUEVFxWQ+prS5rtTTIyWTUjgsxWJSIGC7KgAApmRSwWLHjh2aN2+eVq5cabqe8pBISK2tUn//ubFIROrslOJxe3UBADBFWd9jMTw8rB07dmjt2rWaPn3S936Wr0RCamzMDBWSNDDgj09gFQgAgEKVdbA4cOCATp48qebm5lzUU9pc11+pGOu2lpGxtjZ/HgAARSjrYHHbbbfJ8zwtWbIkF/WUtp6e0SsV5/M8qa/PnwcAQBHiWSH5lEyanQcAQIEhWORTOGx2HgAABYZgkU+xmL/7Y7xOpY4jRaP+PAAAihDBIp8CAX9LqTQ6XIy87uignwUAoGgRLPItHpe6u6WqqszxSMQfp48FAKCI0YjChnhcamig8yYAoOQQLGwJBKT6ettVAABgFJdCAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGDMdNsFADa5rtTTIyWTUjgsxWJSIGC7KgAoXgQLlK1EQmptlfr7z41FIlJnpxSP26sLAIoZl0JQlhIJqbExM1RI0sCAP55I2KkLAIodwQJlx3X9lQrPG/3eyFhbmz8PAJAdggXKTk/P6JWK83me1NfnzwMAZIdggbKTTJqdBwA4h2CBshMOm50HADiHYIGyE4v5uz8cZ+z3HUeKRv15AIDsECxQdgIBf0upNDpcjLzu6KCfBQBMBsECZSkel7q7paqqzPFIxB+njwUATA4NslC24nGpoYHOmwBgEsECZS0QkOrrbVcBAKWDSyEAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGOyDhYDAwO6++67NWfOHM2cOVNXX321Dh8+nIvaAADABLmudPCg1NXlf3ddO3VMz2byhx9+qOXLl2vFihXat2+f5s6dq+PHj2vWrFm5qg8AAFxAIiG1tkr9/efGIhGps1OKx/NbS1bB4qmnnlI0GtWOHTvOji1atMh4UQAAYGISCamxUfK8zPGBAX+8uzu/4SKrSyHPP/+8rrvuOq1evVrz5s3Ttddeq6effvoLfyadTiuVSmV8AQCAqXNdf6Xi86FCOjfW1pbfyyJZBYv33ntPW7du1eWXX679+/frxz/+sdavX6+dO3eO+zPt7e0KhUJnv6LR6JSLBgAAUk9P5uWPz/M8qa/Pn5cvjueNlXPGNmPGDF133XV65ZVXzo6tX79evb29+tvf/jbmz6TTaaXT6bOvU6mUotGoBgcHFQwGp1A6AADlratLuuuuC8/btUtas2Zqn5VKpRQKhS54/s5qxSIcDuuqq67KGLvyyit18uTJcX+moqJCwWAw4wsAAExdOGx2nglZBYvly5frnXfeyRg7duyYFi5caLQoAABwYbGYv/vDccZ+33GkaNSfly9ZBYv7779fr776qp588kmdOHFCu3bt0vbt29XS0pKr+gAAwDgCAX9LqTQ6XIy87ujw5+VLVsHi+uuv1549e9TV1aWlS5fqiSeeUEdHh5qamnJVHwAA+ALxuL+ltKoqczwSyf9WUynLmzdNmOjNHwAAYOJc19/9kUz691TEYmZXKiZ6/s6qQRYAAChMgYBUX2+7Ch5CBgAADCJYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADBmuu0CAACT4LpST4+UTErhsBSLSYGA7aoAggUAFJ1EQmptlfr7z41FIlJnpxSP26sLEJdCAKC4JBJSY2NmqJCkgQF/PJGwUxfw/xEsAKBYuK6/UuF5o98bGWtr8+cBlhAsAKBY9PSMXqk4n+dJfX3+PMASggUAFItk0uw8IAcIFgBQLMJhs/OAHCBYAECxiMX83R+OM/b7jiNFo/48wBKCBQAUi0DA31IqjQ4XI687OuhnAasIFgBQTOJxqbtbqqrKHI9E/HH6WMAyGmQBQLGJx6WGBjpvoiARLACgGAUCUn297SqAUbgUAgAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAY0rjsemuK/X0SMmkFA5LsZj/SGEAAJBXxR8sEgmptVXq7z83FolInZ1SPG6vLgAAylBxXwpJJKTGxsxQIUkDA/54ImGnLgAAylTxBgvX9VcqPG/0eyNjbW3+PAAAkBfFGyx6ekavVJzP86S+Pn8eAADIi+INFsmk2XkAAGDKsgoWjz/+uBzHyfiqqanJVW1fLBw2Ow8AAExZ1rtCamtrdeDAgXN/wHRLG0tiMX/3x8DA2PdZOI7/fiyW/9oAAChTWaeC6dOn6ytf+UouaslOIOBvKW1s9EPE+eHCcfzvHR30swAAII+yvsfi+PHjmj9/vi677DI1NTXp5MmTXzg/nU4rlUplfBkTj0vd3VJVVeZ4JOKP08cCAIC8cjxvrOsIY9u3b5/OnDmjK664QslkUhs3btTAwICOHj2qysrKMX/m8ccf18aNG0eNDw4OKhgMTr7y89F5E1niPxkAyE4qlVIoFLrg+TurYPF5H330kRYuXKgtW7bo3nvvHXNOOp1WOp3OKCwajZoNFkAWaNYKANmbaLCY0p2Xl156qZYsWaITJ06MO6eiokIVFRVT+RjAmJFmrZ+P0yPNWrmCBgBTM6U+FmfOnNG7776rMFs6UQRo1goAuZdVsHjwwQd16NAh/fvf/9Yrr7yiO+64Q4FAQGvWrMlVfYAxNGsFgNzL6lJIf3+/1qxZo//+97+aO3eubrzxRr366quaO3duruoDjKFZKwDkXlbBYvfu3bmqA8g5mrUCQO4V77NCgCyNNGsd6Z/2eY4jRaM0awWAqSBYoGyMNGuVRocLmrUCgBkEC5QVmrUCQG5ZeoIYYE88LjU00HkTAHKBYIGyFAhI9fW2qwCA0sOlEAAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxky3XQAAAPC5rtTTIyWTUjgsxWJSIGC7quwQLAAAKACJhNTaKvX3nxuLRKTOTiket1dXtrgUAgCAZYmE1NiYGSokaWDAH08k7NQ1GQQLAAAscl1/pcLzRr83MtbW5s8rBgQLAAAs6ukZvVJxPs+T+vr8ecWAYAEAgEXJpNl5thEsAACwKBw2O882ggUAABbFYv7uD8cZ+33HkaJRf14xIFgAAGBRIOBvKZVGh4uR1x0dxdPPgmABAIBl8bjU3S1VVWWORyL+eDH1saBBFgAABSAelxoa6LwJAAAMCQSk+nrbVUwNl0IAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMZMt10AAECS60o9PVIyKYXDUiwmBQK2qwKyRrAAANsSCam1VervPzcWiUidnVI8bq8uYBK4FAIANiUSUmNjZqiQpIEBfzyRsFMXMEkECwCwxXX9lQrPG/3eyFhbmz8PKBIECwCwpadn9ErF+TxP6uvz5wFFYkrBYvPmzXIcR21tbYbKAYAykkyanQcUgEkHi97eXm3btk3Lli0zWQ8AlI9w2Ow8oABMKlicOXNGTU1NevrppzVr1izTNQFAeYjF/N0fjjP2+44jRaP+PKBITCpYtLS0aOXKlbrllltM1wMA5SMQ8LeUSqPDxcjrjg76WaCoZN3HYvfu3XrjjTfU29s7ofnpdFrpdPrs61Qqle1HAkDpisel7u6x+1h0dNDHAkUnq2DR19en1tZWvfjii7rooosm9DPt7e3auHHjpIoDgLIQj0sNDXTeRElwPG+sDdRj27t3r+644w4FzvuP3XVdOY6jadOmKZ1OZ7wnjb1iEY1GNTg4qGAwaOAQAABArqVSKYVCoQuev7Nasbj55pt15MiRjLF169appqZGDz300KhQIUkVFRWqqKjI5mMAAECRyipYVFZWaunSpRljF198sebMmTNqHAAAlB86bwIAAGOm/HTTgwcPGigDAACUAlYsAACAMVNesSg4rsuWLQAALCmtYJFIjN1kprOTJjMAAORB6VwKSSSkxsbRjyAeGPDHEwk7dQEAUEZKI1i4rr9SMVavr5GxtjZ/HgAAyJnSCBY9PaNXKs7neVJfnz8PAADkTGkEi2TS7DwAADAppREswmGz8wAAwKSURrCIxfzdH44z9vuOI0Wj/jwAAJAzpREsAgF/S6k0OlyMvO7ooJ8FSpbrSgcPSl1d/nfuUwZgS2kEC8nvU9HdLVVVZY5HIv44fSxQohIJqbpaWrFCuusu/3t1NTusAdjheN5YezRzZ6LPc580Om+ijIy0b/n8v+KRhToyNQBTJnr+Lr1gAZQJ1/VXJsbbae04/oLd+++TrQFM3UTP36VzKQQoM7RvAVCICBZAkaJ9C4BCRLAAihTtWwAUIoIFUKRo3wKgEBEsgCJF+xYAhYhgARQx2rcAKDTTbRcAYGricamhgfYtAAoDwQIoAYGAVF9vuwoA4FIIAAAwiGABAACMIVgAAABjCBYAAMAYggUAADCGXSEAgILhumydLnYECwBAQUgkpNbWzKf2RiJ+h1mavRUPLoUAAKxLJKTGxsxQIUkDA/54ImGnLmSPYAEAsMp1/ZUKzxv93shYW5s/D4WPYAEAsKqnZ/RKxfk8T+rr8+eh8BEsAABWJZNm58EuggUAwKpw2Ow82EWwAABYFYv5uz8cZ+z3HUeKRv15KHwECwCAVYGAv6VUGh0uRl53dNDPolgQLAAA1sXjUne3VFWVOR6J+OP0sSgeNMgCABSEeFxqaKDzZrEjWAAACkYgINXX264CU0GwAFA4eFAEUPQIFgAKAw+KAEoCN28CsI8HRQAlg2ABwC4eFAGUFIIFALt4UARQUggWAOziQRFASSFYALCLB0UAJYVgAcAuHhQBlBSCBQC7CvxBEa4rHTwodXX537mHFPhiBAsA9hXogyISCam6WlqxQrrrLv97dTW7X4Ev4njeWHu8cieVSikUCmlwcFDBYDCfH13Y6DgIFNS/g5HWGp//P+TIIgoPxkK5mej5m2BRCOg4CBQU1/VXJsbbBes4/j/R998n/6N8TPT8zaUQ2+g4CBQcWmsAk0ewsImOg0BBorUGMHkEC5v4tQgoSLTWACaPYGETvxYBBYnWGsDkZRUstm7dqmXLlikYDCoYDKqurk779u3LVW2lj1+LgIJU4K01gIKWVbCIRCLavHmzXn/9dR0+fFjf+ta31NDQoLfffjtX9ZU2fi0CClaBttYACt6Ut5vOnj1bv/zlL3XvvfdOaD7bTT9nZFeIlHkTJ5vlgYJQQK01AKsmev6ePtkPcF1Xv//97/Xxxx+rrq5u3HnpdFrpdDqjMJxn5NeisfpYdHQQKgDLAgGpvt52FUDxyDpYHDlyRHV1dfr00091ySWXaM+ePbrqqqvGnd/e3q6NGzdOqciSF49LDQ38WgQAKHpZXwr57LPPdPLkSQ0ODqq7u1vPPPOMDh06NG64GGvFIhqNcikE+cNaNgBMWd5aet9yyy1avHixtm3bZrQwwAjapQOAEXlr6T08PJyxIgEUDNqlA0DeZXWPxcMPP6zbb79dCxYs0NDQkHbt2qWDBw9q//79uaoPmJwLtUt3HL9dekMDl0UAwKCsgsXp06f1wx/+UMlkUqFQSMuWLdP+/ft166235qo+YHKyaZfOLf8AYExWweLXv/51ruoAzKJdOgBYwbNCUJpolw4AVhAsUJpolw4AVhAsUJp4ihQAWEGwQOniKVIAkHeTflYIUBRolw4AeUWwQOnjKVIAkDdcCgEAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADG5L3zpud5kqRUKpXvjwYAAJM0ct4eOY+PJ+/BYmhoSJIUjUbz/dEAAGCKhoaGFAqFxn3f8S4UPQwbHh7WqVOnVFlZKefzj7OWn4ii0aj6+voUDAbzWZp15Xrs5XrcUvkee7ket1S+x16uxy2VzrF7nqehoSHNnz9f06aNfydF3lcspk2bpkgkcsF5wWCwqP8CpqJcj71cj1sq32Mv1+OWyvfYy/W4pdI49i9aqRjBzZsAAMAYggUAADCm4IJFRUWFHnvsMVVUVNguJe/K9djL9bil8j32cj1uqXyPvVyPWyq/Y8/7zZsAAKB0FdyKBQAAKF4ECwAAYAzBAgAAGEOwAAAAxhRMsHjppZe0atUqzZ8/X47jaO/evbZLyov29nZdf/31qqys1Lx58/Td735X77zzju2y8mLr1q1atmzZ2aYxdXV12rdvn+2y8m7z5s1yHEdtbW22S8m5xx9/XI7jZHzV1NTYLisvBgYGdPfdd2vOnDmaOXOmrr76ah0+fNh2WTlXXV096u/ccRy1tLTYLi3nXNfVo48+qkWLFmnmzJlavHixnnjiiQs+a6PY5b3z5ng+/vhjXXPNNWpublY8HrddTt4cOnRILS0tuv766/W///1PjzzyiG677Tb985//1MUXX2y7vJyKRCLavHmzLr/8cnmep507d6qhoUFvvvmmamtrbZeXF729vdq2bZuWLVtmu5S8qa2t1YEDB86+nj69YP43lDMffvihli9frhUrVmjfvn2aO3eujh8/rlmzZtkuLed6e3vluu7Z10ePHtWtt96q1atXW6wqP5566ilt3bpVO3fuVG1trQ4fPqx169YpFApp/fr1tsvLHa8ASfL27NljuwwrTp8+7UnyDh06ZLsUK2bNmuU988wztsvIi6GhIe/yyy/3XnzxRe+mm27yWltbbZeUc4899ph3zTXX2C4j7x566CHvxhtvtF1GQWhtbfUWL17sDQ8P2y4l51auXOk1NzdnjMXjca+pqclSRflRMJdC4BscHJQkzZ4923Il+eW6rnbv3q2PP/5YdXV1tsvJi5aWFq1cuVK33HKL7VLy6vjx45o/f74uu+wyNTU16eTJk7ZLyrnnn39e1113nVavXq158+bp2muv1dNPP227rLz77LPP9Nxzz6m5uXnMh1CWmhtuuEF/+ctfdOzYMUnSW2+9pZdfflm333675cpyq/TXIIvI8PCw2tratHz5ci1dutR2OXlx5MgR1dXV6dNPP9Ull1yiPXv26KqrrrJdVs7t3r1bb7zxhnp7e22Xklff+MY39Oyzz+qKK65QMpnUxo0bFYvFdPToUVVWVtouL2fee+89bd26VQ888IAeeeQR9fb2av369ZoxY4bWrl1ru7y82bt3rz766CPdc889tkvJiw0bNiiVSqmmpkaBQECu62rTpk1qamqyXVpu2V4yGYvK9FLIj370I2/hwoVeX1+f7VLyJp1Oe8ePH/cOHz7sbdiwwfvyl7/svf3227bLyqmTJ0968+bN8956662zY+VyKeTzPvzwQy8YDJb85a8vfelLXl1dXcbYfffd533zm9+0VJEdt912m/ed73zHdhl509XV5UUiEa+rq8v7xz/+4f3mN7/xZs+e7T377LO2S8spViwKxE9+8hP96U9/0ksvvTShx8qXihkzZuirX/2qJOnrX/+6ent71dnZqW3btlmuLHdef/11nT59Wl/72tfOjrmuq5deekn/93//p3Q6rUAgYLHC/Ln00ku1ZMkSnThxwnYpORUOh0etxF155ZX6wx/+YKmi/Pvggw904MABJRIJ26XkzU9/+lNt2LBB3//+9yVJV199tT744AO1t7eX9EoVwcIyz/N03333ac+ePTp48KAWLVpkuySrhoeHlU6nbZeRUzfffLOOHDmSMbZu3TrV1NTooYceKptQIUlnzpzRu+++qx/84Ae2S8mp5cuXj9pGfuzYMS1cuNBSRfm3Y8cOzZs3TytXrrRdSt588sknmjYt81bGQCCg4eFhSxXlR8EEizNnzmT81vL+++/r73//u2bPnq0FCxZYrCy3WlpatGvXLv3xj39UZWWl/vOf/0iSQqGQZs6cabm63Hr44Yd1++23a8GCBRoaGtKuXbt08OBB7d+/33ZpOVVZWTnqHpqLL75Yc+bMKfl7ax588EGtWrVKCxcu1KlTp/TYY48pEAhozZo1tkvLqfvvv1833HCDnnzySX3ve9/Ta6+9pu3bt2v79u22S8uL4eFh7dixQ2vXri2L7cUjVq1apU2bNmnBggWqra3Vm2++qS1btqi5udl2abll+1rMiL/+9a+epFFfa9eutV1aTo11zJK8HTt22C4t55qbm72FCxd6M2bM8ObOnevdfPPN3p///GfbZVlRLvdY3HnnnV44HPZmzJjhVVVVeXfeead34sQJ22XlxQsvvOAtXbrUq6io8Gpqarzt27fbLilv9u/f70ny3nnnHdul5FUqlfJaW1u9BQsWeBdddJF32WWXeT/72c+8dDptu7Sc4rHpAADAGPpYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjPl/tJfzxrA82cYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_state = np.array([[5.0, 8.0, 1.5]])\n",
    "plotActualDynamicsAndNNPredictions(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376fe4b",
   "metadata": {},
   "source": [
    "<span style=\"color: #8080ff\">**QUESTION 5**: </span> Why does the NN predictions seem better for $x_0 = [1.0, 1.0, 0.5]$ compared with $x_0 = [5.0, 8.0, 1.5]$? (*Hint:* take a look at the file *DatasetGeneration.ipynb* to see how the trajectories are created for the dataset, in specific the initial points.)\n",
    "\n",
    "**Answer:** Our dataset contains trajectories starting from the set $[-3, 3] \\times [-3, 3] \\times [-3, 3]$, and they tend to stay close to this set. Therefore, when we test our NN dynamics for $x_0 = [5.0, 8.0, 1.5]$, we are testing for a trajectory that is too far from the training set, and hence it is \"natural\" that results are worse. This can be seen as an example of a larger problem in ML called *distributional shift*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8217c5d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mg\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1742d36",
   "metadata": {},
   "source": [
    "## Varying the hyperparamters\n",
    "After seeing the example, we now have the goal of building a function which allows us to evaluate the performance for a certain implementation of a neural network, given a number of hyperparamters. The goal is to implement a neural network with two hidden layers, using relu activation in all but the last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(hidden_neurons_1, hidden_neurons_2):\n",
    "    '''\n",
    "    hidden_neurons_1: The amount of hidden neurons in the first hidden layer, int\n",
    "    hidden_neurons_2: The amount of hidden neurons in the second hidden layer, int\n",
    "    '''\n",
    "    # Build the neural networ model class\n",
    "    class Neural_network(nn.Module): \n",
    "        def __init__(self): \n",
    "            super(Neural_network, self).__init__()\n",
    "            self.fc1 = nn.Linear(784, hidden_neurons_1, bias = True)  \n",
    "            self.fc2 = nn.Linear(hidden_neurons_1, hidden_neurons_2, bias = True) \n",
    "            self.fc3 = nn.Linear(hidden_neurons_2, 10, bias = True) \n",
    "\n",
    "        def forward(self, x): \n",
    "            x = torch.flatten(x, start_dim = 1) \n",
    "            x = self.fc1(x) \n",
    "            x = F.relu(x) \n",
    "            x = self.fc2(x) \n",
    "            x = F.relu(x) \n",
    "            x = self.fc3(x) \n",
    "            x = F.softmax(x, dim = -1) \n",
    "            return x\n",
    "        \n",
    "    # Initialize the neural network\n",
    "    torch.manual_seed(0)\n",
    "    net = Neural_network()\n",
    "    return net\n",
    "    \n",
    "def train_network(x_train_norm_short, x_test_norm,\n",
    "                  y_train_cat_short, y_test, \n",
    "                  epochs, batch_size, \n",
    "                  net, optimizer):\n",
    "    '''\n",
    "    x_train_norm:     Normalized training data, np.array\n",
    "    x_test_norm:      Normalized testing data, np.array\n",
    "    y_train_cat:      One hot encoding of training labels, np.array\n",
    "    y_test:           Testing labels, np.array\n",
    "    epochs:           Number of epochs the model is trained for, int\n",
    "    batch_size:       Batch size inside the epoch, int\n",
    "    optimizer:        The optimizer used for training the neural network\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    # Define the neural network loss function\n",
    "    loss_func = nn.MSELoss()\n",
    "    \n",
    "    # Train the neural network\n",
    "    net.train() \n",
    "    batches = int(np.floor(len(y_train_cat)/batch_size)) \n",
    "    Index = np.arange(len(y_train_cat)) \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        np.random.shuffle(Index) \n",
    "        loss_epoch = 0\n",
    "        for batch in range(batches):\n",
    "            Index_batch = Index[batch * batch_size:(batch + 1) * batch_size]\n",
    "            x_batch = torch.from_numpy(x_train_norm[Index_batch]) \n",
    "            y_batch = torch.from_numpy(y_train_cat[Index_batch].astype('float32')) \n",
    "            optimizer.zero_grad()\n",
    "            y_batch_pred = net(x_batch)\n",
    "            loss = loss_func(y_batch_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss\n",
    "    \n",
    "        loss_epoch /= batches\n",
    "        print('Loss for epoch {}/{}: {:0.4e}'.format(epoch,epochs, loss_epoch) )\n",
    "        \n",
    "    # Predict test set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = net(torch.from_numpy(x_test_norm))\n",
    "    y_test_pred = y_test_pred.detach().numpy()\n",
    "    \n",
    "    # Exctract labels\n",
    "    y_pred = y_test_pred.argmax(axis = 1)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900c50e",
   "metadata": {},
   "source": [
    "After defining the function, we can now vary the hyperparameters. Here the goal is to vary two of the hyperparameters:\n",
    "<ul>\n",
    "  <li>The optimizer used: Use Adam as well as SDG</li>\n",
    "  <li>The batch size used: Use 10, 100, 1000, 10000 </li>\n",
    "  <li>The hidden_neurons_2 used: Use 10, 50, 100 </li>\n",
    "</ul>  \n",
    "Meanwhile, epochs = 100 and hidden_neurons_1 = 100 are to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281be2c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[269], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(Index)\n\u001b[0;32m     13\u001b[0m num_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60000\u001b[39m\n\u001b[1;32m---> 14\u001b[0m x_train_norm_short \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_norm\u001b[49m[Index[:num_train_samples]]\n\u001b[0;32m     15\u001b[0m y_train_cat_short \u001b[38;5;241m=\u001b[39m y_train_cat[Index[:num_train_samples]]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, opti \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Optimizers):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "Optimizers = ['sdg', 'adam'] #, optimizer_adam]\n",
    "Batch_size = [10, 100, 1000, 10000]\n",
    "Hidden_neurons_2 = [10, 50, 100]\n",
    "\n",
    "epochs = 100\n",
    "hidden_neurons_1 = 100\n",
    "\n",
    "Accuracy = np.zeros((len(Optimizers), len(Batch_size), len(Hidden_neurons_2)))\n",
    "\n",
    "# For faster computation, we will use less data_samples\n",
    "Index = np.arange(len(y_train))\n",
    "np.random.shuffle(Index)\n",
    "num_train_samples = 60000\n",
    "x_train_norm_short = x_train_norm[Index[:num_train_samples]]\n",
    "y_train_cat_short = y_train_cat[Index[:num_train_samples]]\n",
    "\n",
    "\n",
    "for i, opti in enumerate(Optimizers):\n",
    "    for j, batch_size in enumerate(Batch_size):\n",
    "        for k, hidden_neurons_2 in enumerate(Hidden_neurons_2):\n",
    "            net = build_network(hidden_neurons_1, hidden_neurons_2)\n",
    "            \n",
    "            if opti == 'sdg':\n",
    "                optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "            else:\n",
    "                optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "                \n",
    "            accuracy = train_network(x_train_norm_short, x_test_norm,  \n",
    "                                     y_train_cat_short, y_test, \n",
    "                                     epochs, batch_size, \n",
    "                                     net, optimizer)\n",
    "        \n",
    "            Accuracy[i,j,k] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8791b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.3407 0.3407 0.3407]\n",
      "  [0.1307 0.1307 0.1307]\n",
      "  [0.0979 0.0979 0.0979]]\n",
      "\n",
      " [[0.9748 0.9748 0.9748]\n",
      "  [0.9744 0.9744 0.9744]\n",
      "  [0.9706 0.9706 0.9706]]]\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263905c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
